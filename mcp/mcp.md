了解しました。MCP（Model Context Protocol）をLLMで作成するためのプロンプトに役立つように、MCPの仕様とそれに関連する各種ライブラリの使い方をまとめた資料を調査・作成します。

少々お待ちください。完了したら内容をご報告します。

# MCP（Model Context Protocol）ガイドドキュメント

## MCPの定義と目的  
MCP（Model Context Protocol）とは、AIアプリケーションが様々なデータソースやツールから**コンテキスト**（文脈情報）を取得・共有するためのオープン標準プロトコルです ([Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction#:~:text=MCP%20is%20an%20open%20protocol,different%20data%20sources%20and%20tools))。Anthropic社によって2024年に提唱され、「AIアプリにおけるUSB-Cポート」にたとえられています ([Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction#:~:text=MCP%20is%20an%20open%20protocol,different%20data%20sources%20and%20tools))。つまり、従来は各AIアプリケーションと各種ツール/データソースの組み合わせごとに個別の接続実装が必要でしたが、MCPにより統一的なインタフェースが提供されます ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=Think%20of%20it%20like%20USB,effort%20across%20teams%2C%20inconsistent%20implementations))。その結果、**M × N**の複雑な統合作業（M個のAIアプリ×N個のツール）を、**M + N**の標準化された統合に簡素化することが可能になります ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=Think%20of%20it%20like%20USB,effort%20across%20teams%2C%20inconsistent%20implementations))。MCPの目的は、LLMを活用したエージェントやチャットボットが**外部データや機能に安全かつ双方向にアクセスできるようにする**ことです ([Introducing the Model Context Protocol \ Anthropic](https://www.anthropic.com/news/model-context-protocol#:~:text=The%20Model%20Context%20Protocol%20is,that%20connect%20to%20these%20servers))。これにより、LLMの回答精度向上やユーザーニーズへの的確な対応が期待できます。

**MCPのコア要素:** MCPは大きくホスト（Host）、クライアント（Client）、サーバ（Server）の三者で構成されるクライアント-サーバアーキテクチャを採用しています ([Introduction - Model Context Protocol](https://modelcontextprotocol.io/introduction#:~:text=At%20its%20core%2C%20MCP%20follows,can%20connect%20to%20multiple%20servers)) ([Visual Guide to Model Context Protocol (MCP)](https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol#:~:text=At%20its%20core%2C%20MCP%20follows,can%20connect%20to%20multiple%20servers))。

- **Host（ホスト）:** ユーザーと対話するAIアプリケーション本体です。例えばClaudeのデスクトップアプリ、IDE上のAIアシスタント、あるいは独自開発のエージェントなどが該当します ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=,AI%20model%20via%20the%20client))。ホストにはLLM（大規模言語モデル）が組み込まれており、その内部でMCPクライアントが動作してサーバと接続します。  
- **Client（クライアント）:** ホスト内で動作し、特定のMCPサーバと1対1で接続するコンポーネントです ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=,AI%20model%20via%20the%20client))。クライアントは対応するサーバとの通信を管理し、サーバが提供するコンテキスト（ツールやデータ）をホスト内のLLMに渡します。ホストアプリケーションが起動すると必要な数のクライアントが生成され、それぞれ対応するサーバとハンドシェイクを行います。  
- **Server（サーバ）:** 外部ツールやデータソースへのインタフェースを標準化された形で公開する軽量なプログラムです ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=,AI%20model%20via%20the%20client))。各MCPサーバは特定の機能領域を担当し、ホストからのリクエストに応じて必要なデータ取得や操作を実行します。サーバは後述する**ツール**・**リソース**・**プロンプト**という要素をエクスポートし、クライアントからの呼び出しに応じてLLMにそれらを提供します。

 ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction)) 上図はMCPの基本アーキテクチャを示しています。ホスト内のLLMエージェントがMCPクライアントを通じてMCPサーバ（複数可）と通信し、サーバは背後の外部サービス（例：天気API、メール送信機能、データベースなど）にアクセスしてLLMに必要な情報や操作手段を提供します。クライアント-サーバ間の通信は**トランスポート層**を介して行われ、例えば標準入出力（stdio）やHTTP + SSE（Server-Sent Events）などが使用されます ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=The%20transport%20layer%20manages%20the,servers%2C%20supporting%20multiple%20transport%20mechanisms))。通信プロトコルにはJSON-RPC 2.0が採用されており、リクエスト、結果、エラーといったメッセージ種別が定義されています ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=Message%20Types))。これによりクライアントはサーバへ関数呼び出し等のリクエストを送り、サーバは結果を返すという双方向通信が確立されます。

**サーバが提供する要素:** MCPサーバはLLMに対して以下の3種類の要素を標準化して提供します ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=The%20current%20components%20of%20MCP,servers%20include))。

- **Tools（ツール）:** *モデル制御*の機能群です。LLM（モデル）側から呼び出すことができる関数のようなもので、特定のアクションを実行します ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=The%20current%20components%20of%20MCP,servers%20include))。例えば「天気情報を取得する」「メールを送信する」「データベースを更新する」といった操作をツールとして定義できます。LLMはこれらツールを適切なタイミングで呼び出すことで、通常の対話だけでは実現できない外部操作を行えるようになります。  
- **Resources（リソース）:** *アプリケーション制御*のデータ群です。主に読み取り専用のデータソースを指し、LLMに追加の知識や文脈を与えるための情報です ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=1.%20Tools%20%28Model,Selected%20before%20running%20inference))。REST APIにおけるGETエンドポイントのような位置付けで、ファイルの内容、データベースのレコード、他のサービスから取得したデータなどが含まれます。LLMはリソースを参照することで最新の情報やドメイン知識を回答に活用できます。  
- **Prompts（プロンプト）:** *ユーザー制御*のテンプレート群です。これはLLMがツールやリソースを効果的に使うための定型プロンプトや指示を事前に用意したものです ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=1.%20Tools%20%28Model,Selected%20before%20running%20inference))。例えば「ドキュメントQA用のプロンプト」「会話のまとめ（要約）のプロンプト」「JSON形式で回答するプロンプト」等が用意され、必要に応じてLLMに提示されます。プロンプトによりLLMの動作を最適化し、一貫性のある振る舞いをさせることが可能です。

 ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction)) 上図はMCPクライアントとサーバの役割、およびツール・リソース・プロンプトの概要を示したものです。MCPクライアントは必要に応じて**ツールの呼び出し**、**リソースの問い合わせ**、**プロンプトの差し込み**を行い、MCPサーバ側はそれらを**外部に公開**しています。ツールはLLMから呼ばれる計算やアクション、リソースはアプリ内で管理されるデータ、プロンプトはユーザーが管理するテンプレートという位置付けになっており、それぞれに典型例（例えばツール:「検索実行」や「メッセージ送信」、リソース:「ファイル内容」や「APIレスポンス」、プロンプト:「ドキュメントQ&A」や「出力をJSONで整形する指示」など）が示されています。  

**通信とハンドシェイク:** MCPではクライアントとサーバが最初に「**機能の握手 (capability exchange)**」を行います。ホスト起動時に各クライアントは接続されたサーバに対し、自身の対応プロトコルバージョンや要求を送り、サーバ側は提供可能な**ツール**・**リソース**・**プロンプト**の一覧や仕様を応答します ([Visual Guide to Model Context Protocol (MCP)](https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol#:~:text=Image)) ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=1,based%20on))。例えばWeather APIのサーバであれば、「利用可能なツール: `get_forecast(場所)`・`get_alerts(州)`、利用可能なリソース: 気象データ（只読）、プロンプトテンプレート: なし」といった**機能カタログ**を返します。ホスト側（クライアント受信側）はこの情報を基に、ユーザーの入力やLLMからの要求に応じてどのサーバのどの機能を使うかを動的に判断します。実際のLLM推論中にツールが必要と判断されれば、クライアントは該当ツールを持つサーバに**ツール呼び出しリクエスト**を送信し、サーバは処理を実行して**結果**を返します ([Model Context Protocol (MCP) an overview](https://www.philschmid.de/mcp-introduction#:~:text=the%20server%20offers,and%20gets%20the%20result))。この結果をLLMが受け取り、回答生成の続きに役立てる、という流れになります。こうしたプロトコルにより、LLMは自力でAPIの詳細をすべて理解していなくても、MCPを介して**必要な機能を即座に利用できる**ようになるのです ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=Why%20Not%20Just%20Give%20the,LLM%20Access%20to%20the%20API)) ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=However%2C%20in%20practice%2C%20this%20method,%EF%B8%8F))。

## MCPで可能になるユースケース例  
MCPを活用することで、従来のLLM単体では難しかった高度な機能や協調動作が実現できます。ここでは代表的なユースケースをいくつか紹介します。

- **マルチエージェント協調:** MCPにより**複数のAIエージェントやツールが共通のプロトコルで接続**できるため、マルチエージェントシステムでの協調動作が容易になります。例えば、あるホスト内で複数のMCPクライアントを立ち上げ、それぞれが異なる専門MCPサーバ（例：コーディングアシスタント用サーバ、ドキュメント検索サーバ、計算エンジンサーバなど）と繋がるとします。LLMは各サーバの機能を組み合わせ、あたかも**複数の専門家エージェントが連携している**かのようにユーザー要求に対応できます。MCPはエージェント間のコンテキスト共有も可能にします。共通のコンテキスト層を介して複数のモデルやサービスが情報を交換できるため、異なるAIサービス間でユーザーの過去履歴や設定を共有することもできます ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=,refine%20responses%20based%20on%20past))。これにより、たとえばチャットボットとスケジュール管理AIが連携してユーザーの会話文脈と予定データを突合しながら応答するといった高度な協調が実現します。

- **長文履歴コンテキストの制御:** LLMの**コンテキストウィンドウ制限**を超えた長い対話履歴や大量データにも、MCPを通じて対処できます。MCPは**ステートフル（状態保持型）のコンテキストレイヤー**を導入し、単発のAPI呼び出しではなく継続的な文脈を維持する設計になっています ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=Model%20Context%20Protocol%20,and%20act%20autonomously%20over%20time))。具体的には、ユーザーとの対話履歴やユーザー設定・プロファイル情報などをセッションごとに保存し、必要に応じて過去の関連情報をLLMに供給します ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=1))。MCPではインタラクションごとに動的に成長する**コンテキストウィンドウ**を用意し、そこに「ユーザーの好み（例：言語やトーン）」「これまでの対話履歴」「環境データ（デバイス情報や所在地）**」などを蓄積します ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=1))。さらに、過去の情報すべてを逐語的に保持するのではなく**要約埋め込み**などで圧縮しながら重要事項を残す工夫もなされています ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=2))。例えば10回分の対話を要約したベクトル表現（意図ベクトル等）を保持し、モデルが過去を参照したい際には該当ベクトルから関連内容を再構築するといったアプローチです ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=2))。これによりトークン消費を抑えつつ、過去文脈をモデルに意識させ続けることができます。結果として、MCP対応のエージェントは**「長期間にわたっても一貫した会話」**が可能となり、ユーザーが繰り返し以前と同じ情報を提供し直す必要が減ります。  

- **LLMのメモリ構造管理:** MCPはLLMに**人間のような記憶システム**を持たせる基盤ともなります ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=MCP%20is%20a%20protocol%20or,It%20focuses%20on))。セッション内の短期的な記憶だけでなく、セッションをまたいだ長期記憶（永続的なユーザーデータやナレッジ）を保持し、必要に応じて取り出せるよう設計されています ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=,responses%20based%20on%20past%20interactions))。例えばMCP搭載の旅行プランAIは、**ユーザーの予算やアレルギー情報、過去のフィードバック**を複数回の会話に渡って覚えておき、次の提案に活かすことができます ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=Example%3A))。これはまるで人間の旅行代理店がリピーター客の好みを覚えているようなもので、AIとの対話が回を重ねるごとによりパーソナライズされていく効果があります。また、**エージェント間でのコンテキスト共有**（Inter-Model Communication）もMCPの特徴で、あるサービスで蓄えた情報を別のサービスのLLMが参照する、といったシナリオも可能になります ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=,across%20different%20services%20and%20applications))。例えばユーザーがスマートスピーカーで話した内容の要旨をMCP経由で車載AIアシスタントが共有し、車中での続きの会話に活かす、といった使い方です。さらに、MCPを使うことでLLMが**対話から学習・適応**する仕組みも構築できます ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=%2A%20Inter,responses%20based%20on%20past%20interactions))。過去のユーザー評価を記憶し、「前回ユーザーが好まなかった提案は避ける」といった**自己改善**をエージェントが行うことも可能です。このようにMCPは、LLMに長短の記憶を与え複雑な知的振る舞いを実現する新しいパラダイムを提供します ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=In%20the%20evolving%20landscape%20of,across%20AI%20models%20and%20systems)) ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=MCP%20is%20a%20protocol%20or,It%20focuses%20on))。

## MCP実現に必要なコンポーネントと設計上のポイント  
MCPを効果的に構築・運用するには、**メモリ管理**、**コンテキストの切り替え**、**スレッド分離**といったコンポーネントや機能についての十分な設計検討が必要です。それぞれについて詳しく解説します。

- **メモリ管理:** MCPシステムにおけるメモリ管理とは、LLMが扱う**対話履歴や知識データをどのように保存・圧縮・検索するか**を指します。LLMにはトークン長の制限があるため、生の履歴全てをそのままプロンプトに詰め込むことはできません。そこで、MCPでは前述の通り**動的コンテキストウィンドウ**と**要約・埋め込み**を組み合わせたアプローチを取ります ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=1)) ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=2))。重要度の高い情報（直近の発言やクリティカルな事実）はそのまま保持し、それ以外の詳細は**ベクトル化**して長期記憶ストアに保存します ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=2))。この長期記憶にはベクトルデータベース（例：PineconeやQdrantなど）を利用し、後から**類似検索**で必要な情報を高速に取り出します。例えば「以前このユーザーは何と発言していたか？」という問いに対し、ベクトル検索で過去発言の類似ベクトルを探し当て、要約を取り出してコンテキストに差し戻すといった処理です。こうした**Semantic Memory（意味記憶）**の手法により、対話が長引いても重要事項は忘れずに参照できます。また、トークン節約のため**メモリのクリアと再構築**も行います。一定以上古い履歴は詳細部分を削除して要約だけ保持する、あるいは会話が一区切りした段階でLLM自身に要約させて次に備える、といった工夫です。MCPを導入することで「どの情報をどれだけ長く覚えておくか」「いつ忘れる（圧縮する）か」というメモリ管理戦略を明示的に設計・実装できます ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=,responses%20based%20on%20past%20interactions))。この結果、モデルはトークン無駄遣いを抑えつつ必要十分なコンテキストを常に得られるようになります。

- **コンテキスト切り替え:** ここでいうコンテキスト切り替えとは、**状況や話題の変化に応じて適切に参照する文脈を変更する**ことです。人間の会話でも、前の話題を踏まえる場合と新しい話題に移る場合で脳内の参照情報（記憶）は切り替わります。LLMエージェントも同様に、ユーザーの意図や会話の流れに応じて**どのメモリを使うか**を切り替える必要があります。MCPではセッションごとにコンテキストを管理しますが、**ユーザーやタスク単位で複数のコンテキストを持てる**ように設計することが望ましいでしょう。例えば一人のユーザーが「仕事の相談」と「趣味の雑談」を同じアシスタントに行う場合、それぞれ別の会話スレッド（コンテキスト）として記憶を分離しておくのが理想です。コンテキスト切り替えを実現するコンポーネントとしては、**コンテキストマネージャ**が考えられます ([The Model Context Protocol (MCP): A New Paradigm in AI Context Management | by Rathan | Mar, 2025 | Medium](https://medium.com/@rathan.george/the-model-context-protocol-mcp-a-new-paradigm-in-ai-context-management-f896942eb0e9#:~:text=1))。これは現在参照すべきセッション情報を選択し、不要になった文脈を閉じたり、新規の文脈領域を開いたりする役割を担います。実装上は、メモリストア内にタグ付けした会話ID単位で履歴を保持し、ユーザーから「新しい話題に移りたい」と指示があれば新IDでメモリを初期化する、といった方法が取れます。また**マルチエージェント間のコンテキスト共有**においても、どの情報を共有するかの選別（切り替え）が必要です。あるエージェントが得た知識の一部だけを別エージェントに引き継ぐ場合、コンテキストマネージャがフィルタリングして渡すといった制御が求められます。このように、MCPのメモリ層は**複数の文脈スレッドを扱える柔軟性**が必要であり、状況に応じた切り替えロジックを設計しておくことが重要です。

- **スレッド分離:** スレッド分離は上記コンテキスト切り替えとも関連しますが、特に**複数の対話や処理を並行して扱う場合にコンテキストが干渉しないようにする**設計を指します。大規模なAIアシスタントプラットフォームでは、同時に複数ユーザーや複数タスクを処理することがあります。その際に、それぞれの会話**スレッド（対話の流れ）**が混ざらないよう、メモリ空間を明確に分離する必要があります。MCPでは基本的にホスト内で**クライアント：サーバ = 1:1**の関係を複数持てるため、例えば「プロジェクトA用の知識サーバ」と「プロジェクトB用の知識サーバ」を別々のクライアント経由で接続し、それぞれ独立したコンテキストを維持するといったことが可能です。この場合、ホスト側のコンテキスト管理としても**会話スレッドID**や**プロジェクト識別子**を用いてメモリを物理的・論理的に隔離します。スレッド分離を適切に行うことで、あるスレッドでのデータ（例えば機密プロジェクトAの情報）が他のスレッド（プロジェクトB）に誤って露出するリスクを減らせます。またエージェントの誤混同も防げるため、応答の一貫性・正確性が向上します。実際にMCPの実装例には、**メモリパスの切り替え**により複数プロジェクトの文脈を整理する「Memory Manager」サーバなどがあります ([
  Memory Manager MCP Server by YU Zongmin | PulseMCP
](https://www.pulsemcp.com/servers/yuzongmin-memory-manager#:~:text=Manages%20and%20switches%20between%20memory,management%20across%20multiple%20AI%20projects))。このようなコンポーネントは、クライアントごとに異なるメモリパス（保存領域）を割り当て、プロジェクトやタスク単位で明確に区別されたコンテキストを提供します ([
  Memory Manager MCP Server by YU Zongmin | PulseMCP
](https://www.pulsemcp.com/servers/yuzongmin-memory-manager#:~:text=Manages%20and%20switches%20between%20memory,management%20across%20multiple%20AI%20projects))。総じて、MCPを実運用する際には**並行処理時のコンテキスト干渉防止策**を組み込むことが欠かせません。

以上のように、MCPの設計ではLLMの拡張機能（外部ツール連携）だけでなく、**記憶**や**文脈**の扱いを構造化することがポイントです。適切なメモリ管理とコンテキスト制御によって、エージェントは長期にわたりユーザーに寄り添った対話や高度な意思決定が可能となります。

## LLM関連主要ライブラリの概要と使用方法  
MCPの構築やコンテキスト管理には、既存のLLM用ライブラリやフレームワークを活用することが効果的です。ここでは代表的なライブラリである**LangChain**, **LlamaIndex**, **Haystack**, **Transformers**について概要と簡単な使用例を紹介します。それぞれのツールを組み合わせることで、MCPの機能実現がより容易になります。

### LangChain – チェーンとエージェントによるLLMアプリ構築  
**LangChain**は、LLMを用いたアプリケーション開発を効率化するためのフレームワーク/ライブラリです ([LangChainとは？主な機能一覧やChatGPTとの連携・Pythonでの使い方を紹介](https://aismiley.co.jp/ai_news/what-is-langchain-chagpt-python/#:~:text=LangChain%E3%81%AF%E3%80%81LLM%E3%81%AE%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E9%96%8B%E7%99%BA%E3%82%92%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E9%80%B2%E3%82%81%E3%80%81%E5%AE%9F%E8%A3%85%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E3%81%A7%E3%81%99%E3%80%82%E4%B8%96%E7%95%8C%E7%9A%84%E3%81%AB%E6%9C%89%E5%90%8D%E3%81%AALLM%E3%81%A7%E3%81%82%E3%82%8BOpenAI%E7%A4%BE%E3%81%AE%E3%80%8CChatGPT%E3%80%8D%E3%82%84G%20oogle%E3%81%8C%E9%96%8B%E7%99%BA%E3%81%97%E3%81%9F%E3%80%8CGemini%E3%80%8D%E3%81%AA%E3%81%A9%E3%80%81%E5%A4%9A%E3%81%8F%E3%81%AELLM%E9%96%8B%E7%99%BA%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E5%8A%B9%E7%8E%87%E5%8C%96%E3%82%84%E7%94%9F%E7%94%A3%E6%80%A7%E3%81%AE%E5%90%91%E4%B8%8A%E3%81%AA%E3%81%A9%E3%82%92%E4%BF%83%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。Harrison Chase氏によって2022年にオープンソース公開され、プロンプトテンプレート管理、対話メモリ、外部ツール呼び出し、エージェント構築などLLM活用に必要な様々なコンポーネントを提供しています。LangChainを使うと、LLMモデル（OpenAIのGPT系、AnthropicのClaude系、Hugging Faceのローカルモデル等）への入力から出力処理までを**チェーン（Chain）**という流れで繋ぎ、**エージェント（Agent）**によるツール利用も簡潔に記述できます ([LangChainとは？主な機能一覧やChatGPTとの連携・Pythonでの使い方を紹介](https://aismiley.co.jp/ai_news/what-is-langchain-chagpt-python/#:~:text=match%20at%20L466%20LangChain%E3%81%AF%E3%80%81LLM%E3%81%AA%E3%81%A9%E3%81%95%E3%81%BE%E3%81%96%E3%81%BE%E3%81%AAAI%E6%8A%80%E8%A1%93%E3%82%92%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A7%E3%80%81%E8%A4%87%E9%9B%91%E3%81%8B%E3%81%A4%E9%AB%98%E5%BA%A6%E3%81%AA%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E9%96%8B%E7%99%BA%E3%82%84%E6%A7%8B%E7%AF%89%E3%82%92%E6%94%AF%E6%8F%B4%E3%81%97%E3%81%BE%E3%81%99%E3%80%82LangChain%E3%81%A7%E3%81%AF%E7%8F%BE%E5%9C%A8%E3%80%81%E3%83%87%E3%83%BC%20%E3%82%BF%E3%81%AE%E5%8F%96%E5%BE%97%E3%83%BB%E5%87%A6%E7%90%86%E3%81%8B%E3%82%89LLM%E3%81%B8%E3%81%AE%E5%85%A5%E5%8A%9B%E3%80%81LLM%E3%81%AE%E5%9B%9E%E7%AD%94%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E5%87%A6%E7%90%86%E3%81%A8%E3%81%84%E3%81%A3%E3%81%9F%E5%A4%9A%E5%B2%90%E3%81%AB%E3%82%8F%E3%81%9F%E3%82%8B%E6%A9%9F%E8%83%BD%E3%82%92%E6%8F%90%E4%BE%9B%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82)) ([LangChainとは？主な機能一覧やChatGPTとの連携・Pythonでの使い方を紹介](https://aismiley.co.jp/ai_news/what-is-langchain-chagpt-python/#:~:text=LangChain%E3%81%AF%E3%80%81%E7%94%9F%E6%88%90AI%E3%82%92%E3%82%88%E3%82%8A%E5%8A%B9%E6%9E%9C%E7%9A%84%E3%81%AB%E6%B4%BB%E7%94%A8%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AB%E5%BD%B9%E7%AB%8B%E3%81%A4%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A7%E3%81%99%E3%80%82%E5%85%AC%E9%96%8B%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E5%A4%9A%E3%81%8F%E3%81%AELLM%E3%81%A8%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%81%8C%E5%8F%AF%E8%83%BD%E3%81%A7%E3%80%81Python%E3%81%A8%E3%81%84%E3%81%86%E6%B1%8E%E7%94%A8%E6%80%A7%E3%81%AE%E9%AB%98%20%E3%81%84%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E8%A8%80%E8%AA%9E%E3%82%92%E7%94%A8%E3%81%84%E3%81%A6%E3%80%81LLM%E7%94%A8%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%82%92%E9%96%8B%E7%99%BA%E3%83%BB%E5%AE%9F%E8%A3%85%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82))。特にLangChainのMemory機能は、チャットボットの履歴を管理するのに便利です。「ConversationBufferMemory」などのクラスを用いると、過去の一定数のメッセージを自動でプロンプトに含め、長い対話でも直近の文脈を維持した応答が可能になります。LangChainは**複数のツールを統合したエージェント**を構築する用途でも強力で、例えばWeb検索や電卓など事前定義ツールを呼び出すエージェントを数行のコードで実装できます。以下にLangChainを用いた簡単な会話アプリの例コードを示します。

```python
# LangChainを用いた対話エージェントの例
from langchain import OpenAI, ConversationChain
from langchain.memory import ConversationBufferMemory

# 対話履歴を保存するメモリを用意（直近のやり取りを格納）
memory = ConversationBufferMemory(k=5)  # 直近5発言を保持
llm = OpenAI(model_name="text-davinci-003", temperature=0.7)  # OpenAIのGPTモデルを使用
conversation = ConversationChain(llm=llm, memory=memory)

# ユーザーとの対話を実行
user_input = "こんにちは、今日は調子はどう？"
response = conversation.run(user_input)
print(response)
```

上記では、OpenAIの言語モデルを使用し、ConversationChainにメモリを組み込んでいます。`conversation.run(...)`を呼ぶたびにユーザー入力が履歴に追加され、次回呼び出し時には直近の履歴（ここでは最大5発言）がプロンプトに含まれます。LangChainはこのようにシンプルなAPIで**コンテキスト維持**を実現でき、MCPで目指す長期対話の実装に役立ちます。また、LangChainは**外部ツールとの連携**も可能です。例えば計算ツールやウェブ検索ツールをロードし、`initialize_agent`関数でLLMと組み合わせることで、LLMが自動的にツールを使って回答するエージェントを構築できます（ReAct手法などを用いたエージェント実装）。総じてLangChainはMCPクライアント側のロジックを組むのに適した高レベルフレームワークと言えます。

### LlamaIndex – 外部データのインデックス化と長文コンテキスト管理  
**LlamaIndex**（旧称 GPT Index）は、LLMと外部データ（ドキュメントやナレッジベース）をつなぐためのデータフレームワークです ([LangChainとは？すぐに使えるLLMのフレームワークを紹介 | アンドエンジニア](https://and-engineer.com/articles/ZPAfUhQAACUAiQbo#:~:text=LlamaIndex%E3%81%A8LangChain%E3%81%AE%E9%81%95%E3%81%84))。簡単に言えば、テキストやドキュメント群を前処理・要約し、LLMが扱いやすい形（ベクトルインデックスやツリー構造など）で保持・検索するためのライブラリです。LlamaIndexを使うと、大量のドキュメントから**必要な情報を検索してLLMに提供**する「Retrieval Augmented Generation (RAG)」の仕組みを構築できます。例えば企業内部のWikiやマニュアル文書をインデックス化し、ユーザーからの質問に対して該当箇所を検索・参照しながらLLMが回答する、といった高度なQAシステムを比較的容易に実装できます。LlamaIndex自体にもエージェント機能やメモリ機能が含まれ始めていますが、本質的な強みは**柔軟なインデックスとクエリエンジン**です。

基本的な使い方として、まずドキュメント読み込み（各種データローダー対応）、次にインデックス構築（ベクトルストア、テキスト要約ツリーなど方式選択可）、最後にクエリ実行という流れになります。以下はディレクトリ内の文書群を読み込んでベクトルインデックスを作成し、質問応答するコード例です。

```python
from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex

# ローカルのドキュメント群を読み込み
documents = SimpleDirectoryReader("data/my_docs").load_data()

# ベクトルストアインデックスを構築（デフォルトはOpenAIのEmbeddingを使用）
index = GPTVectorStoreIndex.from_documents(documents)

# インデックスからクエリエンジンを取得し、質問に答える
query_engine = index.as_query_engine()
query = "このシステムのデザイン原則は何ですか？"
response = query_engine.query(query)
print(response)
```

上記コードでは、指定フォルダ内の文章を全てベクトル埋め込みしインデックス化しています。`query_engine.query()`に質問を与えると、内部で類似度検索により関連文書を見つけ、それらをもとにLLMが回答を生成します。結果として、ベースとなるLLM（例: GPT-4）の知識に文書から得た最新情報を組み合わせた回答が得られます。LlamaIndexは他にもグラフ構造インデックスやリストインデックスなど様々なモードがあり、用途に応じたコンテキスト管理が可能です。

MCPにおいてLlamaIndexは、例えば**MCPサーバ側で知識ベースを提供する**用途に活用できます。社内データ検索用のMCPサーバを実装し、その中でLlamaIndexを使ってユーザー問い合わせに関連する文書を検索する、といったシナリオです。また、LangChainとの統合も容易で、LlamaIndexはLangChain用の**Tool**インタフェースや**Memoryモジュール**を提供しています ([Using with Langchain  - LlamaIndex  v0.10.18.post1](https://docs.llamaindex.ai/en/v0.10.18/community/integrations/using_with_langchain.html#:~:text=Using%20with%20Langchain%20))。つまり、LangChainエージェントの一部としてLlamaIndexの検索機能を「ツール」として組み込み、エージェントが自発的に必要なデータを取得できるようになります ([Using with Langchain  - LlamaIndex  v0.10.18.post1](https://docs.llamaindex.ai/en/v0.10.18/community/integrations/using_with_langchain.html#:~:text=Use%20any%20data%20loader%20as,a%20Langchain%20Tool))。この統合により、LangChainエージェントがLlamaIndex経由で大容量データからリアルタイムに情報を引き出す**オンデマンドデータアクセス**が実現できます ([Using with Langchain  - LlamaIndex  v0.10.18.post1](https://docs.llamaindex.ai/en/v0.10.18/community/integrations/using_with_langchain.html#:~:text=Use%20any%20data%20loader%20as,a%20Langchain%20Tool))。以上のように、LlamaIndexはLLMの長期記憶や知識検索を担うコンポーネントとしてMCPシステムの重要な一翼を担います。

### Haystack – 検索志向のQAシステムフレームワーク  
**Haystack**は、ドイツのdeepset社が開発するオープンソースのエンドツーエンドNLPフレームワークで、特に**質問応答（QA）や検索補助生成 (RAG)** に強みを持っています ([Haystackのチュートリアルを全部やってみる](https://zenn.dev/kun432/scraps/52700695e1eb58#:~:text=Haystack%E3%81%AF%E3%80%81%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%88Large%20Language%20Models%EF%BC%9ALLM%EF%BC%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%A6%E3%80%81%E6%A7%98%E3%80%85%E3%81%AA%E6%A4%9C%E7%B4%A2%E3%83%A6%E3%83%BC%E3%82%B9%E3%82%B1%E3%83%BC%E3%82%B9%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%A7%E3%81%8D%E3%82%8B%E3%80%81%E3%83%91%E3%83%AF%E3%83%95%E3%83%AB%E3%81%A7%E3%83%97%E3%83%AD%E3%83%80%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%20%E5%AF%BE%E5%BF%9C%E3%81%AE%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%82%A8%E3%83%B3%E3%83%89%E3%83%84%E3%83%BC%E3%82%A8%E3%83%B3%E3%83%89%E3%81%AE%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A7%E3%81%99%E3%80%82%E6%A4%9C%E7%B4%A2%E6%94%AF%E6%8F%B4%E7%94%9F%E6%88%90%EF%BC%88RAG%EF%BC%89%E3%80%81%E8%B3%AA%E5%95%8F%E5%BF%9C%E7%AD%94%E3%80%81%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%96%87%E6%9B%B8%E6%A4%9C%E7%B4%A2%E3%81%AE%E3%81%84%E3%81%9A%E3%82%8C%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B%E5%A0%B4%E5%90%88%E3%81%A7%E3%82%82%E3%80%81Ha%20ystack%E3%81%AE%E6%9C%80%E5%85%88%E7%AB%AF%E3%81%AELLM%E3%81%A8NLP%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%80%81%E7%8B%AC%E8%87%AA%E3%81%AE%E6%A4%9C%E7%B4%A2%E3%82%A8%E3%82%AF%E3%82%B9%E3%83%9A%E3%83%AA%E3%82%A8%E3%83%B3%E3%82%B9%E3%82%92%E6%8F%90%E4%BE%9B%E3%81%97%E3%80%81%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%81%8C%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%82%AF%E3%82%A8%E3%83%AA%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82Hay,stack%E3%81%AF%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E6%96%B9%E5%BC%8F%E3%81%A7%E6%A7%8B%E7%AF%89%E3%81%95%E3%82%8C%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%81OpenAI%E3%80%81Cohere%E3%80%81SageMaker%E3%80%81%E3%81%AA%E3%81%A9%E6%9C%80%E9%AB%98%E3%81%AE%E6%8A%80%E8%A1%93%E3%81%A8%E3%80%81Hugging))。Haystackを使うと、ドキュメントストア（検索対象のデータベース）、リトリーバー（検索器）、リーダー/生成器（回答生成モデル）といったコンポーネントを組み合わせて、柔軟なQAパイプラインを構築できます ([Haystackのチュートリアルを全部やってみる](https://zenn.dev/kun432/scraps/52700695e1eb58#:~:text=%3E%20Haystack%20is%20an%20end,applications%20solving%20your%20use%20case)) ([Haystackのチュートリアルを全部やってみる](https://zenn.dev/kun432/scraps/52700695e1eb58#:~:text=Haystack%E3%81%AF%E3%80%81%E5%A4%A7%E8%A6%8F%E6%A8%A1%E3%81%AA%E6%96%87%E6%9B%B8%E3%82%B3%E3%83%AC%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%82%A4%E3%83%B3%E3%83%86%E3%83%AA%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%81%AB%E5%8B%95%E4%BD%9C%E3%81%99%E3%82%8B%E3%80%81%E3%83%97%E3%83%AD%E3%83%80%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%E5%AF%BE%E5%BF%9C%E3%81%AE%20LLM%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%80%81%E6%A4%9C%E7%B4%A2%E6%94%AF%E6%8F%B4%E7%94%9F%E6%88%90%EF%BC%88RAG%EF%BC%89%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%80%81%E6%9C%80%E5%85%88%E7%AB%AF%E3%81%AE%E6%A4%9C%E7%B4%A2%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%20%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%20%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%AE%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF%20%E3%81%A7%E3%81%99%E3%80%82Haystack%E3%81%AE%E8%A9%B3%E7%B4%B0%E3%81%A8%E5%8B%95%E4%BD%9C%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E5%AD%A6%E3%82%93%E3%81%A7%E3%81%84%E3%81%8D%E3%81%BE%E3%81%97%E3%82%87%E3%81%86%E3%80%82))。Haystackは様々なベクトルデータベースやモデル（Transformersベースのモデル含む）と連携でき、企業内の大規模文書コレクションに対する**高度な質問応答システム**を実現する用途でよく使われています。

Haystackの特長の一つは、**パイプライン構築の容易さ**です。Haystackには標準で`Pipeline`クラスが用意されており、決められた順序でコンポーネントを流すシンプルなパイプラインや、条件分岐・マルチステップを含むカスタムパイプラインを構築できます。またHaystackはエージェント機能も実験的に提供しており、LLMがツールを使って回答を導くような動的振る舞いも可能です ([Haystackのチュートリアルを全部やってみる](https://zenn.dev/kun432/scraps/52700695e1eb58#:~:text=,a%20Pipeline%20as%20a%20Tool))。しかし一般的には、Haystackは**ユーザーの質問に対して関連ドキュメントを検索し、必要なら生成系モデルで回答を生成する**というQAフローで使われます。

簡単な例として、Haystackでベクトル検索＋生成モデルによるオープンQAを行うコードを示します。

```python
from haystack.document_stores import InMemoryDocumentStore
from haystack.nodes import BM25Retriever, PromptNode
from haystack.pipelines import Pipeline

# ドキュメントストアとリトリーバの準備
document_store = InMemoryDocumentStore()
document_store.write_documents([
    {"content": "東京は日本の首都であり...", "meta": {"source": "wiki"}},
    {"content": "大阪は日本で2番目に大きな都市...", "meta": {"source": "wiki"}}
])
retriever = BM25Retriever(document_store=document_store)

# プロンプトノード（生成モデル）を準備 - FLAN-T5などの事前学習モデルを使用
generator = PromptNode(model_name_or_path="google/flan-t5-base", default_prompt_template="question-answering")

# パイプラインを構築: Retrieverで検索しGeneratorで回答生成
pipe = Pipeline()
pipe.add_node(component=retriever, name="Retriever", inputs=["Query"])
pipe.add_node(component=generator, name="Generator", inputs=["Retriever"])

# 質問に対してパイプラインを実行
result = pipe.run(query="日本の首都はどこですか？")
print(result["answers"][0].answer)
```

この例では、まず簡易なBM25リトリーバで文書ストアから関連文書を検索し、その結果をPromptNode（内部で生成系モデルを使用）に渡して質問への回答を生成しています。`Pipeline`にノードとして追加されたRetrieverとGeneratorが順次実行され、最終的に回答が得られます。Haystackは他にもDensePassageRetrieverやEmbeddingRetriever、TransformersベースのReader（抽出型QAモデル）など多彩なノードを提供しており、要件に合わせて組み替えるだけで**抽出型QA**から**生成型QA**まで対応できます。

MCPとの関係で言えば、Haystackは**既存の検索・QA機能をMCPサーバとして提供する**形で活用できます。例えばHaystackで構築した社内QAシステムをMCPサーバ化し、LLMエージェントがそのサーバのAPI（ツール）を呼び出して回答を得る、といった統合が考えられます。LangChain等と直接統合する公式機能はありませんが、LangChainのカスタムツールとしてHaystackのパイプライン実行を呼び出すことも可能でしょう。Haystackは**生のNLP処理パイプライン**を柔軟に設計できる点で、LLMの前段/後段のテキスト処理を強化したい場合に有用です。特にドキュメント検索や社内ナレッジの活用が重要なMCPユースケースでは、Haystackによる信頼性の高い検索結果をLLMに与えることで回答の正確性向上に寄与します ([Haystackのチュートリアルを全部やってみる](https://zenn.dev/kun432/scraps/52700695e1eb58#:~:text=Haystack%E3%81%AF%E3%80%81%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%EF%BC%88Large%20Language%20Models%EF%BC%9ALLM%EF%BC%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%A6%E3%80%81%E6%A7%98%E3%80%85%E3%81%AA%E6%A4%9C%E7%B4%A2%E3%83%A6%E3%83%BC%E3%82%B9%E3%82%B1%E3%83%BC%E3%82%B9%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%A7%E3%81%8D%E3%82%8B%E3%80%81%E3%83%91%E3%83%AF%E3%83%95%E3%83%AB%E3%81%A7%E3%83%97%E3%83%AD%E3%83%80%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%20%E5%AF%BE%E5%BF%9C%E3%81%AE%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%82%A8%E3%83%B3%E3%83%89%E3%83%84%E3%83%BC%E3%82%A8%E3%83%B3%E3%83%89%E3%81%AE%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A7%E3%81%99%E3%80%82%E6%A4%9C%E7%B4%A2%E6%94%AF%E6%8F%B4%E7%94%9F%E6%88%90%EF%BC%88RAG%EF%BC%89%E3%80%81%E8%B3%AA%E5%95%8F%E5%BF%9C%E7%AD%94%E3%80%81%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%96%87%E6%9B%B8%E6%A4%9C%E7%B4%A2%E3%81%AE%E3%81%84%E3%81%9A%E3%82%8C%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B%E5%A0%B4%E5%90%88%E3%81%A7%E3%82%82%E3%80%81Ha%20ystack%E3%81%AE%E6%9C%80%E5%85%88%E7%AB%AF%E3%81%AELLM%E3%81%A8NLP%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%80%81%E7%8B%AC%E8%87%AA%E3%81%AE%E6%A4%9C%E7%B4%A2%E3%82%A8%E3%82%AF%E3%82%B9%E3%83%9A%E3%83%AA%E3%82%A8%E3%83%B3%E3%82%B9%E3%82%92%E6%8F%90%E4%BE%9B%E3%81%97%E3%80%81%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%81%8C%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%82%AF%E3%82%A8%E3%83%AA%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82Hay,stack%E3%81%AF%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E6%96%B9%E5%BC%8F%E3%81%A7%E6%A7%8B%E7%AF%89%E3%81%95%E3%82%8C%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%81OpenAI%E3%80%81Cohere%E3%80%81SageMaker%E3%80%81%E3%81%AA%E3%81%A9%E6%9C%80%E9%AB%98%E3%81%AE%E6%8A%80%E8%A1%93%E3%81%A8%E3%80%81Hugging))。

### Transformers – Hugging Faceによるモデル実行とカスタムLLM  
**Transformers**ライブラリ（Hugging Face社）は、最先端の大規模言語モデルを容易に利用するためのオープンソースフレームワークです ([ Transformers](https://huggingface.co/docs/transformers/ja/index#:~:text=Transformers%20%E3%81%AF%E6%9C%80%E5%85%88%E7%AB%AF%E3%81%AE%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E7%B0%A1%E5%8D%98%E3%81%AB%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%97%E3%81%A6%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8BAPI%E3%81%A8%E3%83%84%E3%83%BC%E3%83%AB%E3%82%92%E6%8F%90%E4%BE%9B%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A7%E8%A8%88%E7%AE%97%E3%82%B3%E3%82%B9%E3%83%88%E3%81%A8%E4%BA%8C%E9%85%B8%E5%8C%96%E7%82%AD%20%E7%B4%A0%E3%81%AE%E6%8E%92%E5%87%BA%E9%87%8F%E3%82%92%E5%89%8A%E6%B8%9B%E3%81%A7%E3%81%8D%E3%80%81%E3%81%BE%E3%81%9F%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AB%E8%A6%81%E6%B1%82%E3%81%95%E3%82%8C%E3%82%8B%E6%99%82%E9%96%93%E3%81%A8%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%92%E7%AF%80%E7%B4%84%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82%20%E3%81%93%E3%82%8C%E3%82%89%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AF%E4%BB%A5%E4%B8%8B%E3%81%AE%E3%82%88%E3%81%86%E3%81%AA%E7%95%B0%E3%81%AA%E3%82%8B%E3%83%A2%E3%83%80%E3%83%AA%E3%83%86%E3%82%A3%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E4%B8%80%E8%88%AC%E7%9A%84%E3%81%AA%E3%82%BF%E3%82%B9%E3%82%AF%E3%82%92%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%81%97%E3%81%BE%E3%81%99%3A))。事前学習済みモデルのダウンロード・実行を行うAPIやツールを提供し、テキスト生成・質問応答・分類など様々なNLPタスク用のモデルを統一的に扱えます ([ Transformers](https://huggingface.co/docs/transformers/ja/index#:~:text=Transformers%20%E3%81%AF%E6%9C%80%E5%85%88%E7%AB%AF%E3%81%AE%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E7%B0%A1%E5%8D%98%E3%81%AB%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89%E3%81%97%E3%81%A6%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8BAPI%E3%81%A8%E3%83%84%E3%83%BC%E3%83%AB%E3%82%92%E6%8F%90%E4%BE%9B%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A7%E8%A8%88%E7%AE%97%E3%82%B3%E3%82%B9%E3%83%88%E3%81%A8%E4%BA%8C%E9%85%B8%E5%8C%96%E7%82%AD%20%E7%B4%A0%E3%81%AE%E6%8E%92%E5%87%BA%E9%87%8F%E3%82%92%E5%89%8A%E6%B8%9B%E3%81%A7%E3%81%8D%E3%80%81%E3%81%BE%E3%81%9F%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AB%E8%A6%81%E6%B1%82%E3%81%95%E3%82%8C%E3%82%8B%E6%99%82%E9%96%93%E3%81%A8%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%92%E7%AF%80%E7%B4%84%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82%20%E3%81%93%E3%82%8C%E3%82%89%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AF%E4%BB%A5%E4%B8%8B%E3%81%AE%E3%82%88%E3%81%86%E3%81%AA%E7%95%B0%E3%81%AA%E3%82%8B%E3%83%A2%E3%83%80%E3%83%AA%E3%83%86%E3%82%A3%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E4%B8%80%E8%88%AC%E7%9A%84%E3%81%AA%E3%82%BF%E3%82%B9%E3%82%AF%E3%82%92%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%81%97%E3%81%BE%E3%81%99%3A))。Transformersライブラリを使うことで、OpenAIやAnthropicのAPIだけでなく、ローカル環境で動作するLLM（例: LLaMA系列モデルやBloomなど）を自前でロードして対話システムを構築できます。MCPの文脈では、Transformersは**LLMそのものの実行基盤**として重要です。LangChainやLlamaIndexがLLMへのインターフェースを抽象化しているのに対し、Transformersは実際のモデル読み込み・推論処理を担います。

Transformersには高レベルな`pipeline`インターフェースがあり、数行のコードでテキスト生成やQAを試すことができます。また、より細かく制御したい場合は`AutoModel`や`AutoTokenizer`を用いてモデルと言語処理用トークナイザを読み込み、PyTorch/TensorFlow上で推論します。以下はTransformersのテキスト生成パイプラインを用いて日本語文章を生成する例です。

```python
from transformers import pipeline

# テキスト生成用のパイプラインを初期化（日本語対応の事前学習モデルを指定）
generator = pipeline("text-generation", model="rinna/japanese-gpt2-medium")

# プロンプトを与えてテキスト生成
result = generator("今日は良い天気なので", max_length=50, do_sample=True)
print(result[0]['generated_text'])
```

このコードでは、`rinna/japanese-gpt2-medium`という日本語GPT-2モデルを使い、「今日は良い天気なので」に続く文章を生成しています。`pipeline`が内部でトークナイズからモデル推論、デコードまで処理して結果を返します。モデルやパラメータを変えれば、**会話モデル**としてDialoGPTやLLaMAを使ったチャットボットを作ることもできます。Transformersは大規模言語モデルだけでなく、画像生成や音声認識などのモデルも統一的に扱えますが、ここではLLMに焦点を当てます。

MCPでTransformersを活用する場面としては、**独自モデルを組み込んだLLMクライアント/サーバ**の構築が挙げられます。例えば社内で微調整した日本語特化のLLMを用いる場合、Transformersでそのモデルを読み込みMCPホスト内のLLMとして利用できます。また、Hugging Faceのモデルリポジトリから必要なモデル（対話特化、長文対応など）を入手し差し替えることで、MCPエージェントの性能や応答スタイルをカスタマイズ可能です。LangChainなども内部ではTransformersやそのラッパーを利用してモデル実行していますが、Transformersを直接使えば**低レベルの最適化**（GPU並列やINT8量子化による高速化等）も行えます。したがって、MCPを自前環境で構築する際には、Transformersを使って最適なLLMを動かしつつLangChainやLlamaIndexで周辺を補強する、というアプローチが効果的です。

## ライブラリ間の組み合わせ方と適用パターン  
最後に、上述した各種ライブラリを組み合わせてMCPの機能を実現するパターンについて考えてみます。単一のフレームワークでは賄えない部分も、複数のツールを統合することで高度なエージェントシステムが構築できます。

- **Retrieval Augmented Generation (RAG) パターン:** これは**LlamaIndexやHaystackを用いた外部知識の検索**と**LLMの統合**パターンです。LangChainを使う場合、LlamaIndexで構築したインデックスをLangChainのツールとして組み込み、エージェントが質問に応じて自動でそのツール（データ検索）を呼び出すことができます ([Using with Langchain  - LlamaIndex  v0.10.18.post1](https://docs.llamaindex.ai/en/v0.10.18/community/integrations/using_with_langchain.html#:~:text=Using%20with%20Langchain%20)) ([Using with Langchain  - LlamaIndex  v0.10.18.post1](https://docs.llamaindex.ai/en/v0.10.18/community/integrations/using_with_langchain.html#:~:text=Use%20any%20data%20loader%20as,a%20Langchain%20Tool))。具体的には、ユーザ質問に対してエージェントがまずLlamaIndexツールで社内データを検索し、その結果をもとに最終回答を生成する、という流れです。Haystackの場合も、LangChainのカスタムツールを実装してHaystackパイプラインを呼び出すことで、類似の統合が可能です。RAGパターンでは**信頼性の高い情報取得**と**流暢な回答生成**の両立が図れます。MCPサーバとして見れば、知識検索専用サーバを一つ立てておき（内部でLlamaIndex/Haystack駆動）、ホストのLLMエージェントが必要に応じてそのサーバのツールAPIを叩くという構成も考えられます。これによりLLMは自身のパラメータに保持していない情報でも、外部データソースから補完して回答することができます。

- **ツール使用エージェントパターン:** LangChainのエージェント機能とMCPの親和性は高く、LangChainからMCP経由のツール群を呼び出すことも可能です。実際にLangChainには**MCPアダプタ**が提供されており、MCPサーバ上のツールをLangChainエージェントのツールとしてロードできます ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=match%20at%20L752%20GitHub%20,com))。例えば前述の「天気情報サーバ」「計算サーバ」をMCPで動かしつつ、LangChainのエージェント（ReAct型など）がそれらをツールとして使えるよう設定できます。こうすると、LangChainエージェントはユーザ質問に応じて「計算が必要だから計算サーバのaddツールを呼ぶ」「天気を聞かれたから天気サーバのget_forecastツールを呼ぶ」といった判断を自動で行い、最終的に統合された回答を返せます ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=async%20with%20MultiServerMCPClient%28%20%7B%20,py%20file)) ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=,agent))。このように**LangChain + MCP**の組み合わせは、LangChainの高レベル推論制御とMCPの多様な外部機能接続を両取りするアプローチです。実装上は、`langchain_mcp_adapters`というモジュールを使ってMCPクライアントを初期化し、接続された複数サーバのツール一覧をLangChainエージェントに提供する形になります ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=from%20langchain_mcp_adapters,PromptTemplate%20from%20dotenv%20import%20load_dotenv)) ([ Understanding Model Context Protocol: A Deep Dive into Multi-Server LangChain Integration | by Plaban Nayak | The AI Forum | Mar, 2025 | Medium](https://medium.com/the-ai-forum/understanding-model-context-protocol-a-deep-dive-into-multi-server-langchain-integration-3d038247e0bd#:~:text=agent%20%3D%20create_react_agent%28model%2C%20client,messages))。MCPによって統一されたインタフェースがあるおかげで、LangChainから見れば通常のツールと同様に扱える点が利点です。

- **ローカルモデル×フレームワークのパターン:** Transformersなどでローカルにモデルを動かしつつ、LangChainやHaystackを組み合わせるパターンです。例えば、社内で独自に訓練した日本語特化のLLMをTransformersでロードし、そのモデルをLangChainのLLMクラス（例えばHuggingFacePipelineなど）としてラップして使うことができます。LangChainはOpenAI API以外にもHugging Faceモデルを扱える仕組みがあるため、ローカルLLM＋メモリ＋ツールというエージェントをネット接続なしでも構築可能です。また、Haystackも生成ノードにローカルモデル（例えばFlan-T5やGPT-NeoX系）を指定できますので、自前モデルを活用したRAGパイプラインを作成できます。MCPを社内ネットワーク内で自己完結的に運用したい場合、このローカルモデル活用は重要です。外部APIに頼らず自社内で完結するLLMエージェントは、セキュリティやプライバシー面でも有利です。Transformersでモデルを最適化（例えば8-bit量子化でメモリ削減）し、LangChainでコンテキスト制御とメモリ管理、LlamaIndexで知識検索、と組み合わせれば、非常に高度なMCPエージェントが構築できます。

- **マルチエージェント分業パターン:** 複雑なタスクを複数の専門エージェントに分担させ、最終的な回答や成果を統合するパターンです。MCPは複数のサーバを並行稼働させられるため、「エージェントAはコード生成担当」「エージェントBは文章要約担当」といった**役割分担エージェント**を作り、それぞれのエージェントが別々のMCPサーバ経由でツールを使うように設計できます。ホスト側では、それらエージェントからの回答を統合したり、あるいは片方の結果をもう片方に渡したりといったオーケストレーションを行います。この際LangChainのシーケンシャルチェーンやエージェントをネストする機能を使うと、ひとつのフレームワーク上で複数LLMの連携が比較的簡単に書けます。例えばLangChainで**一問一答エージェント**と**長文要約エージェント**を用意し、両者を呼び分ける制御フローを作る、といった形です。MCP自体は一つのLLMアシスタントを想定していますが、このように内部でモデルを複数動かしても構いません。重要なのは**コンテキストの共有と隔離を適切に管理**することです。必要な場合は前述のコンテキストマネージャを拡張して、エージェント間で一部のメモリを共有する（例えば共通のユーザープロファイルは共有し、個別の作業メモリは分離する）などの戦略をとります。複数エージェントの協調は非常に強力ですが調整も難しいため、段階的に実装・検証すると良いでしょう。

以上、MCPの概念から具体的なライブラリ活用まで包括的に解説しました。MCPはまだ新しい標準ですが、「LLMを外界とつなぐ統一ハブ」として今後重要度を増す技術です。LangChainやLlamaIndexといったエコシステムと組み合わせることで、開発者は高度なAIアシスタントを効率よく構築できます。適切なメモリ設計とライブラリ活用により、マルチエージェント協調や長期コンテキスト保持といったこれまで難しかったユースケースが実現可能になります。本ガイドを踏まえて、是非MCPを用いた次世代のLLMアプリケーション開発に挑戦してみてください。

